# Outlier Detection in Python Brett Kennedy

## Chapter 1 - What is Outlier Detection?

Outlier detection is finding unusual items - very subjective

Many outliers are not interesting, but many interesting things are outliers

Can usually be applied when you have lots of data

---

> An outlying observation, or outlier, is one that appears to deviate markedly from the other members of the sample in which it occurs. 
> Grubbs in 1969

> An observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data. 
> Barnett and Lewis (1994) used the following definition: 

> An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.
> Hawkins (1980)

> An outlier is that which an outlier detector flags.
> Brett Kennedy (2023)

Combined data that comes from two different data generating processes

---

Point anomalies = single point

Contextual/collection anomalies = unusual given other context, like readings from other sensors OR readings from same sensor in the past

---

## Chapter 1 - Use Cases

### Alerting / Industrial Processes

There will be many issues of some severity, but under a threshold, and these may not trigger any immediate action.

The number of these can be logged - number per day, then alert beyond a certain number per day

### Data Quality

Detecting issues

1. defining rule
2. can also use outlier detection

### Evaluating Segmentation

## Chapter 1 - History

Three stages

1. statistical methods
2. traditional machine learning
3. deep learning

Traditional ML methods are pretty stable for tabular + time series, still better than deep learning

## Chapter 2 - Statistical Methods

### z score

- limits depend on the distribution (how symmetric it is -> whether upper / lower limits are same level)
- not robust to outliers (they affect the calculated mu + std)

Each z-score has a probability (for known distribution)

### interquartile range (IQR)

Preferred to z-score 
- more robust to outliers
- uses median, rather than mean -> more robust

### median absolute deviation (MAD)

### modified z-score

```
modified_z_score = 0.6745 * (x - median) / mad
```

### Masking + Swamping

Masking - outliers causing other outliers not to be detected

Swamping - outliers outliers causing other outliers to be detected when they shouldn't

Masking & swamping can occur with other methods

### Expected & Unexpected Outliers

Two types of outliers

1. extreme but expected
2. so extreme it's unexpected

## Interval vs External Outliers

Internal can only occur in multi-modal distributions or distributions with gaps

Internal can be detected by

- histograms
- KDE
- nearest neighbour methods

These 3 methods are more flexible than the other statistical methods and are often preferred

### Histograms

Count number of samples in bins

### KDE

Similar, except estimates probability density

### KNN

## Categoricals

Count distinct values

Can apply MAD to the count of distinct values to detect outliers

## Multidimensional Outliers

Based on multiple columns

Only two reasons to flag a row
1. the row contains one or more unusual values
2. the row contains one or more combinations of values that is unusual

When a point is outlier in a given dimensionality, it will also be in higher dimensions but not necessarily lower dimensions

Visualizing

- two categorical variables -> heatmap
- two numeric -> scatterplot
- one categorical, one numeric -> boxplot

Boxplots bad with multimodal data

Categorical

- rare combinations based on absolute count
- rare combinations based on marginal probabilities

## Noise vs. Inliers and Outliers

Weak versus strong outliers
- weak = outlier from noise

## Local & Global Outliers

In-distribution and out of distribution outliers

Seems the same idea as expected and unexpected outliers???

## Combining Scores of Univariate Tests

Run simple tests on each feature, and sum across each row
- hard to do fairly (different feature scales etc)
- can scale

## Chapter 3 - ML Outlier Detection

Most ML methods are multivariate

[RnR curse of dimensionality, data sparsity, distances]

Distance measurements unreliable in high dimensions

### Types of Algorithms

1. distance based - knn
2. density based - estimate regions of low density, local outlier factor
3. cluster based

All require scaling of numeric variables

Clustering evaluate each point comapred to its neighbour

Distance / density can look at arbitrary regions

### Density

LOF = local outlier factor
- uses distances to estimate density

Multivariate Gaussian distribution
- estimate mean & std
- calcuate z-score / probability of each observation -> probability of each row (multiply each probability together)
- outlier = rows with low probabilities
- assuming no-correlation & gaussian distributions
- probabilites cannot be inferred well if distribution is not gaussian
- gaussian mixtures is a variation of this

Radius
- how many other points are in a certain radius -> estimate of space density
- radius selection needs to be done manually

KDE
- can be done multidimensionally

### Cluster

Outliers are far away from clusters

Can explicit segment / cluster data before ML

- ie separating wind speeds into separate buckets
- can cluster separately

### Types of Detectors

- how sensitive to contaminated data
- numeric or categorical
- global and/or local outlier detection
- output score or binary label (scores vs. flags)
- training & inference time
- interpretability
- robustness to curse of dimensionality

Novelty detection = trained with very clean data (no anomalies)

Outlier detection = trained with some contamination

## Chapter 4 - Outlier Detection Process

Rules

Good
- fast to execute
- deterministic
- interpretable
- easier to test
- can be audited

Bad
- written by hand (error prone, time consuming)
- can only see what is specifically encoded
- rules can become arbitrarily complex
- non-stationary - can't adapt to changing patterns

Should do both rules and detector based approaches

## In production / ongoing outlier detection

Ensembles help
- can see different patterns
- average through variance in model results
- more reliable final score

Most common reference for outliers is the data itself

# Chapter 5 - Outlier detection using sklearn

## Isolation Forest

Most generally applicable (numeric tabular data)

Isolates anomalous points through recursive division of the data

More similar to ExtraTrees than RandomForest
- bootstraps
- decision trees to split data

Features not interpretable

Tunes model using area under roc curve

Does do some test/train split stuff - TO REVIEW AGAIN

## Gaussian Mixture Models

Represent data with series of multivariate gaussians
- each gaussian represents a data generating process

Clustering or outlier detection

Previous clustering algos are `hard clustering` - each record in exactly one or no cluster

GMM are soft clustering - each record has series of scores, one for each cluster

## BallTree + KDTree

BallTree good/used with methods based on distances

Tree data structures again

# PyOD

## Histogram-based Outlier Score (HBOS)

[RnR - bunch of other detectors]


# Other Libraries

alibi-detect library - https://github.com/SeldonIO/alibi-detect

PyNomaly library - https://github.com/vc1492a/PyNomaly

Extended Isolation Forest - https://github.com/sahandha/eif

## Entropy

Eaisest used with categorical detectors

Entropy measures uncertainty in data

Entropy based outlier detection iteratively removes rows,

## Association Rule mining

More interpretable

## Convex Hull

Assumes one cluster of data

---

LDOF, ODIN, Entropy, Association Rules, Convex Hull, distance metric learning, and NearestSample are straightforward to implement and can all be effective algorithms for outlier detection.


# Chapter 8

Histograms of the feature values, with vertical lines where rows are flagged as outliers

Contour plots
- can be used to eyeball the tightness of the anomaly clusters
- can only show two features at once

Correlation of detector scores
- naive Spearman correlation is bad

Use subset of data to calculate correlation:

```
For example, if two detectors scored a row as 0.02 and 0.42 (both on a scale from 0.0 to 1.0), then the difference is large, but it is not relevant: both consider the record to be an inlier. What’s really relevant is the degree of agreement regarding the outliers, not regarding the inliers. This is the agreement as to which records are outliers and agreement in the actual ranking of the outliers. There are several ways to test this, though there is no single best way. One method is to take only the rows where at least one detector scored the row in their top 5% (or at some such cutoff). We can then calculate the correlation in scores on this subset of the data. Another method is to take the scores from each detector, determine the rank order, and set any scores below the top 5% (or some cutoff) to 0.0. An example of this is shown in listing 8.6. These will consider the actual rank for any high scores and will treat all low scores equivalently, simply as inliers with rank zero. This assumes the scores_df dataframe contains a row for each row in the data and a column for each detector used.
```

Min max scaling:

```
In listing 8.7, we look at another method. Here we apply RobustScaler to the scores from all detectors so that the scores are on the same scale. Scaling preserves more of the distribution of scores than using rank order. In this case, MinMax scaling works poorly as some detectors, particularly LOF, can generate a small number of scores far higher than others, and MinMax scaling will push all other values to zero. Here we set all scaled values below 2.0 to 0.0.
```

We want diversity

## Synthetic data

Says it's important for development & testing

- can insert outliers we expect, and check we detect them

Allows testing beyond 2 features (which most visualization are limited to 2 features)

Doping
- changing existing rows

DopingOutlierTester (https://mng.bz/j0Ka).

Provides a ground truth for testing

`precision at k` metric
- if we know top X will be outliers / we will look at the top X

# Chapter 9 - Specific data types

[RnR - just feature engineering really]

Add distance metrics as features...

# Chapter 10 - Small & large dataset sizes

[RnR]

## Feature Subspaces

Random feature subspaces

Correlation based subspaces

# Chapter 11 - Synthetic data

[RnR]

Faker (https://github.com/xfxf/faker-python)

Simulation

Training classifiers to detect real from fake data

# Collective Outliers

Often when analyzing data, we’re interested in finding not just unusual individual records, but any unusual patterns in the data. 

For this, an important step in outlier detection is searching for what are called collective outliers. 

These are cases in which individual rows are not necessarily unusual but sets of rows are.
